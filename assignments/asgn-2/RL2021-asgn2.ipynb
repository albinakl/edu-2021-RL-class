{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#333333; text-align:center; line-height: 0;\">Reinforcement Learning | Assignment 2</h1>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "This notebook covers a Policy Gradient **REINFORCE** approach.\n",
    "\n",
    "Complete the code snippets given in the Section 3: there are several places to insert your code and string fields for your first and last name. The latter are needed to automatically save the results of the algorithms deployment in .json file. After you did that, please upload the notebook (.ipynb) and .json via https://forms.gle/MWZ4Po2f6hs2s7Ny8.\n",
    "\n",
    "* Problem 2.1 - Swing Up Policy (10 points)\n",
    "* Problem 2.2 - Gradient Calculation (20 points)\n",
    "* Problem 2.3* (additional) - NPG (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Section 1: Theory recap</h2>\n",
    "\n",
    "Let us recall the REINFORCE algorithm from the lecture.\n",
    "\n",
    "<img src=\"PG.png\" alt=\"REINFORCE\" width=75% height=75% />\n",
    "\n",
    "The second problem will be dedicated to the implementation of the function that calculates the right-hand side of the 10th line of the pseudocode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Section 2: OpenAI Pendulum environment</h2>\n",
    "\n",
    "In contrast to the first assumption, this time we will consider an environment with continuous state and action spaces: OpenAI Pendulum https://gym.openai.com/envs/Pendulum-v0/. The overview of the state vector, possible actions and their bounds is given in https://mspries.github.io/jimmy_pendulum.html\n",
    "\n",
    "Let us examine the dynamic behaviour of the Pendulum by applying several simple policies. First, we will implement a wrapper function that will run the simulation for a number of episodes with a given policy and plot the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_episode(policy):\n",
    "    ep_len = 282\n",
    "\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    env._max_episode_steps = ep_len\n",
    "    \n",
    "    observation = env.reset()\n",
    "    reward_history = []\n",
    "    \n",
    "    for t in range(ep_len):  \n",
    "        env.render()\n",
    "        \n",
    "        time.sleep(0.01)\n",
    "        \n",
    "        action = policy(observation)\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        reward_history.append(reward)\n",
    "    \n",
    "    plt.plot(reward_history)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first policy that we'll apply is the policy that applies constant $0.5$ counterclockwise torque. Run it for a number of times in order to explore the reward behaviour under this policy with different initial states. You could increase the torque up to the limit and make the pendulum rotate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_policy(obs):\n",
    "    return [0.5]\n",
    "\n",
    "_ = run_episode(half_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another policy, that will not exhibit such a cyclic behaviour, is the random one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(obs):\n",
    "    return [np.random.random_sample() * 4 - 2]\n",
    "\n",
    "_ = run_episode(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Section 3: Problems</h2>\n",
    "\n",
    "### <font color=\"blue\">Problem 2.1 - Swing Up Policy</font>\n",
    "\n",
    "Implement a policy that stabilizes the pendulum in the upwards position.\n",
    "\n",
    "Thr first policy here is the one that stabilizes the pendulum downwards (check the plot!). The second does the opposite. Please familiarize yourself with the environment (using the links given above) well enough to understand the exact way in which the negative feedback in the first policy stabilizes the pendulum.\n",
    "\n",
    "The approach that you are asked to complete relies on the following:\n",
    "* When the pendulum is in the relatively low position, the policy should destabilize (accelerate) it\n",
    "* When the surrounding of the higher equilibrium is reached, the policy should stabilize the pendulum\n",
    "* It is enough to set negative feedback by the angular velocity for stabilization in the lower equilibrium. However, stabilization in the higher one requires an additional negative feedback term by coordinate: without it the pendulum will slowly move away from the desired position.\n",
    "\n",
    "Your goal is to complete the code below, in patricular:\n",
    "* Set a condition for switching between stabilizing and destabilizing modes for the policy. It could be angle, measured from the desired position (note that it is not in the observation vector, it should be calculated), height of the center of mass, etc.\n",
    "* Set the control coefficients for both ways of torque calculation. Try to understand the relation between them: are they positive/negative, which one has greater value. Generally, the policy should accelerate the pendulum with moderately high torque for the stabilization to be possible.\n",
    "\n",
    "The policy should be capable of stabilizing the pendulum most of the time, at least 4 out of 5 trials, during the given number of episodes. After you implemented it, save the rewards of a single run with the help of the Auto-grading cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stabilizing_policy(obs):\n",
    "    return [- obs[2]]\n",
    "\n",
    "def destabilizing_policy(obs):\n",
    "    return [obs[2]]\n",
    "\n",
    "def swing_up_policy(obs):\n",
    "    ### YOUR SOLUTION BELOW\n",
    "    if (...):\n",
    "        torque = ... * (obs[2] + obs [1])\n",
    "        \n",
    "        return [torque]\n",
    "    \n",
    "    else:\n",
    "        return [... * obs[2]]\n",
    "    ### YOUR SOLUTION ABOVE\n",
    "\n",
    "reward_history = run_episode(stabilizing_policy)\n",
    "#reward_history = run_episode(destabilizing_policy)\n",
    "#reward_history = run_episode(swing_up_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 2.1. Make sure you defined the necessary variable above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "from grading_utilities import AnswerTracker\n",
    "asgn2_answers = AnswerTracker()\n",
    "asgn2_answers.record('problem_2-1', {'reward_history': reward_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Problem 2.2 - Gradient Calculation</font>\n",
    "\n",
    "Examine the code below. Note the way it generalizes and wraps the `swing_up_policy`. Correlate the lines of code of `REINFORCE` with the pseudocode above.\n",
    "\n",
    "Let us briefly outline the main novelties in comparison to the code above.\n",
    "\n",
    "* The control coefficients are given to the policy as a parameter.\n",
    "* A function for the PDF gradient calculation is sketched.\n",
    "* Random noise (by the name of `nrv`) is included in the process. Familiarize yourself with the way it is transferred during the execution.\n",
    "\n",
    "Let us unwrap the latter a little bit. The resultant torque is given by\n",
    "\n",
    "$\\hat{\\tau}(\\vartheta) =\n",
    "\\begin{equation*}\n",
    "    \\begin{cases}\n",
    "      \\vartheta[0] (\\dot{\\theta} + \\sin(\\theta)), \\; condition \\\\\n",
    "      \\vartheta[1] \\dot{\\theta}, \\quad \\quad \\quad \\; \\; \\; otw.\n",
    "    \\end{cases}\n",
    "\\end{equation*}$\n",
    "\n",
    "where $\\vartheta$ is a vector of policy parameters.\n",
    "\n",
    "Adding Gaussian noise leads to the following PDF:\n",
    "\n",
    "$f(\\tau) = \\dfrac{1}{\\sigma \\sqrt{2 \\pi}} e^{ -\\frac{1}{2}\\left(\\dfrac{\\tau - \\hat{\\tau}}{\\sigma}\\right)^2 }$\n",
    "\n",
    "The task is the following:\n",
    "* Insert your policy switching criteria into `parametrized_swing_up_policy` and `param_policy_grad`.\n",
    "* Insert your control coefficients into the initialization of `vartheta` in `REINFORCE`.\n",
    "* Take partial derivatives of $\\ln f(\\tau)$ by the componemts of $\\vartheta$.\n",
    "* Write code for their calculation (using the given variables) in the `param_policy_grad` function\n",
    "* Run the cell (note the flag `visualize`) with and without updating paramenters during the run ( `update_params`). Do it multiple times and compare the performance. Feel free to output any information you need, such as cumulative reward, to plot anything you need. Because of the complex structure (read as nonconvexity) of the reward function by parameters, the performance could change in any direction. The thing that is checked in the task is that the method is indeed working, not that it converges to the optimum.\n",
    "* When you are done, run the code with updating parameters and save the reward history by running the Grading cell below.\n",
    "\n",
    "You could vary the Learning Rate $\\alpha$ it fou need or scale components of $\\vartheta[1]$ relatively to each other if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def parametrized_swing_up_policy(obs, vartheta, s):\n",
    "    #normal random variable\n",
    "    nrv = np.random.normal(0, s, 1)[0]\n",
    "    \n",
    "    ### YOUR SOLUTION ON THE LINE BELOW\n",
    "    if (...):\n",
    "        torque = vartheta[0] * (obs[2] + obs [1]) + nrv\n",
    "        \n",
    "        return [torque], nrv\n",
    "    \n",
    "    else:\n",
    "        return [vartheta[1] * obs[2] + nrv], nrv\n",
    "\n",
    "#x - state\n",
    "#u - action\n",
    "#s - sigma of the normal distribution\n",
    "#nrv - the specific value of the random variable\n",
    "def param_policy_grad(x, s, nrv):\n",
    "    ### YOUR SOLUTION BELOW\n",
    "    if (...):\n",
    "        \n",
    "\n",
    "    else:\n",
    "        \n",
    "    \n",
    "    ### YOUR SOLUTION ABOVE\n",
    "\n",
    "ep_len = 340\n",
    "env = gym.make('Pendulum-v0')\n",
    "env._max_episode_steps = ep_len\n",
    "\n",
    "def REINFORCE(env, update_params, visualize = False):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    ### YOUR SOLUTION BELOW\n",
    "    vartheta = np.array([..., ...])\n",
    "    ### YOUR SOLUTION ABOVE\n",
    "\n",
    "    steps_num    = 20\n",
    "    episodes_num = 30\n",
    "\n",
    "    policy = parametrized_swing_up_policy\n",
    "    alpha = 0.00001\n",
    "\n",
    "    sigma = 0.3\n",
    "    \n",
    "    reward = 0\n",
    "    reward_history = []\n",
    "    \n",
    "    for step in range(steps_num):\n",
    "        Grad = np.array([0.0, 0.0])\n",
    "\n",
    "        acc_reward = 0\n",
    "        policy_PDF_grad = np.array([0.0, 0.0])\n",
    "\n",
    "        for ep in range(episodes_num):        \n",
    "            if (visualize == True):\n",
    "                env.render()\n",
    "\n",
    "            action, nrv = policy(observation, vartheta, sigma)\n",
    "\n",
    "            acc_reward += reward\n",
    "            \n",
    "            reward_history.append(reward)\n",
    "\n",
    "            ppg = param_policy_grad(observation, action, sigma, nrv)\n",
    "            \n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            policy_PDF_grad += ppg\n",
    "\n",
    "        Grad += acc_reward * policy_PDF_grad\n",
    "\n",
    "        if (update_params):\n",
    "            vartheta += alpha * Grad\n",
    "    \n",
    "    return reward_history\n",
    "\n",
    "parametric_policy_reward_history = REINFORCE(env, update_params = True, visualize = True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 2.2. Make sure you defined the necessary variable above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "asgn2_answers.record('problem_2-2', {'reward_history': parametric_policy_reward_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Problem 2.3* (additional) - NPG</font>\n",
    "\n",
    "Copy the code from the Problem 2.2 and modify the gradient step in accordance with the NPG algorithm. Feel free to rewrite code in any way you need. This task is an extra one, so there will bo no guidance. The only requirement (apart from the convergence to the higher equilibrium) is the name of the list with the rewards history for the AnswerTracker to save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION BELOW\n",
    "\n",
    "### YOUR SOLUTION ABOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 2.3. Make sure you defined the necessary variable above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "asgn2_answers.record('problem_2-3', {'reward_history': NPG_reward_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading: Submit your answers</font>\n",
    "Enter your first and last name in the cell below and then run it to save your answers for this assumption to a JSON file. The file is saved next to this notebook. After the file is created, upload the JSON file and the notebook via the form provided in the beginning of the assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_name = \"asgn_2\"\n",
    "first_name = \"\"\n",
    "last_name = \"\"\n",
    "\n",
    "asgn2_answers.save_to_json(assignment_name, first_name, last_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "\n",
    "Reach out to Ilya Osokin (@elijahmipt) on Telegram.\n",
    "\n",
    "## Sources\n",
    "\n",
    "***\n",
    "\n",
    "<sup>[1]</sup> Ng, A. Stanford University, CS229 Notes: Reinforcement Learning and Control.\n",
    "\n",
    "<sup>[2]</sup> Barnabás Póczos, Carnegie Mellon, Introduction To Machine Learning: Reinforcement Learning (Course).\n",
    "\n",
    "<sup>[3]</sup> **Sutton, R. S., Barto, A. G. (2018 ). Reinforcement Learning: An Introduction. The MIT Press.** \n",
    "\n",
    "<sup>[4]</sup> OpenAI: Spinning Up. Retrieved from https://spinningup.openai.com/en/latest/spinningup/rl_intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
