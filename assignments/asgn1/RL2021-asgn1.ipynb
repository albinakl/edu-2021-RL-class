{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#333333; text-align:center; line-height: 0;\">Reinforcement Learning | Assignment 1</h1>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "This notebook covers two classical approaches in RL: **value iteration** and **policy iteration**.\n",
    "\n",
    "Before you start, please install OpenAI gym for Python [here](https://gym.openai.com/docs/).\n",
    "\n",
    "Complete the code snippets given in the Section 3: there are 4 places to insert your code, one string field to enter the algorithm name and string fields for your first and last name. The latter are needed to automatically save the results of the algorithms deployment in .json file. After you did that, please upload the notebook (.ipynb) and .json via https://forms.gle/yJepv3CzfwMRnQULA.\n",
    "\n",
    "* Problem 1.1 - Value Iteration (15 points)\n",
    "* Problem 1.2 - Policy Iteration (15 points)\n",
    "* Problem 1.3 - FrozenLake8x8 (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Section 1: Theory recap</h2>\n",
    "\n",
    "$x$ - state from state space $\\mathbb{X}$\n",
    "\n",
    "$u$ - action from action space $\\mathbb{U}$\n",
    "\n",
    "$R$ - reward function as a random variable ($\\rho$ as a deterministic function)\n",
    "\n",
    "$\\kappa$ - policy (in deterministic case)\n",
    "\n",
    "$\\vartheta$ - policy parameters from parameter space $\\Pi$\n",
    "\n",
    "Discrete time deterministic system (DT):\n",
    "\n",
    "$x_{k+1} = f(x_k, u_k)$\n",
    "\n",
    "$u_k = \\kappa(x_k)$\n",
    "\n",
    "The core problem that is being solved in RL is teaching the agent to behave *optimally* in a specific environment.\n",
    "\n",
    "The numerical definition of what is *optimal* is given by the **objective function** $J$, which is typically maximized in the context of MDPs (Markov Decision Processes):\n",
    "\n",
    "$X_{k+1} \\sim P_{X}\\left(x_{k+1} \\mid x_{u}, u_{n}\\right)$\n",
    "\n",
    "$U_{k} \\sim P_{U}^{\\vartheta}\\left(u_{k} \\mid x_{k}\\right)$\n",
    "\n",
    "The next state and next control are sampled from the respective probability distributions. $P_{X}, P_{U}^{\\vartheta}$ are probability distribution (or density if $\\mathbb{X}$, $\\mathbb{U}$ are continuous) functions (PDF). Policy is represented here by a PDF with parameters $\\vartheta$, but may be a definite function (Markov policy) $ = \\kappa(X_{k})$\n",
    "\n",
    "The goal then is to teach the agent to maximize the *total expected reward* it earns over some time horizon (theoretically, it is an infinite time horizon) by selecting the best action as dictated by the value function. The agent learns to maximize rewards as it transitions from state to state, taking actions in each state.\n",
    "\n",
    "Consider the following $\\infty$-horizon optimal control problem for an MDP:\n",
    "$$\n",
    "J(x) = \\max _{\\vartheta} \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} \\rho\\left(X_{k}, \\kappa\\left(X_{k}\\right)\\right) \\mid X_{0}=x\\right]\n",
    "$$\n",
    "\n",
    "The value function is defined in the following way:\n",
    "\n",
    "$V(x) = \\underset{\\vartheta}{\\max} \\ J^{\\vartheta}(x)$\n",
    "\n",
    "Let us use the denotation $X_{+}^{u} \\sim P_{X}(x \\mid x, u)$ for the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Section 2: OpenAI FrozenLake environment</h2>\n",
    "\n",
    "For this homework we will be exploring agent training in the grid world environment called *FrozenLake*. Read more about it [here](https://gym.openai.com/envs/FrozenLake-v0/). To summarize:\n",
    "* FrozenLake is a 2D grid world\n",
    "* There are 2 variants of the environment: 4x4 and 8x8. We will try both.\n",
    "* There are 4 types of grid cells (S, F, H, G). *S* is the starting point, *G* is the goal, *F* is a frozen surface and *H* is a hole.\n",
    "* If an agent steps onto a slippery surface, it may slip and not end up in the next desired state for which it took an action (think, transition probabilities!).\n",
    "* The rewards are sparse: the agent receives a reward of 1 when reaching the goal and 0 otherwise. If the agent falls into a hole, the episode is over.\n",
    "* Note: we will not be using the slippery version of the environment for simplicity.\n",
    "\n",
    "Familiarize yourself with the code below and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(observation)\n",
    "        if done:\n",
    "            #print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Section 3 - Dynamic Programming algorithms for MDPs</h2>\n",
    "\n",
    "The pseudocode for the general case of Value Iteration and Policy Iteration is given below.\n",
    "\n",
    "For the environments with discrete state and action spaces, such as FrozenLake, it becomes shorter and easier to comprehend, see pp. 74-83 in Chapter 4 of the class text <sup>[3]</sup>. Be aware of the differences in the notation: $s$ instead of $x$, etc.\n",
    "\n",
    "We have implemented a class `MDP`, containing methods that are common to these algorithms.\n",
    "\n",
    "**\\_model_transits_rewards** method implements experience gaining, i.e. performing random actions in order to make $\\infty$-horizon reward prediction possible.\n",
    "\n",
    "The resultant policies converge to the optimal policy in terms of the Functional norm, meaning that $\\lim \\limits_{k \\rightarrow \\infty} \\|V_k - \\hat{V}\\| = \\lim \\limits_{k \\rightarrow \\infty} \\sup \\limits_{x \\in \\mathbb{X}} \\|V_k(x) - \\hat{V}(x)\\| = 0$. However, real-world implementation requires practical termination criteria. Here we stop the procedure when the maximal change of the policy among all the states during one optimization loop does not exceed a certain constant $\\nu$, see pseudocode for details. The value $\\nu$ is a tuning parameter for both algorithms.\n",
    "\n",
    "The initial values for Value Iteration could be set to $0$. Same for the Policy Iteration, but only for the finite state spaces: in continuous case the criteria for the feasible initialization are discussed later in the course.\n",
    "\n",
    "<img src=\"algorithms.png\" alt=\"Policy Iteration\" width=75% height=75% />\n",
    "\n",
    "These algorithms have very similar value-of-state estimation cycle. The difference is that the *value iteration* algorithm iterates over **every** action while calculating the expected reward, and selects action with the maximal value. While the policy iteration calculates expected reward for the **single** action **specified by the policy**.\n",
    "\n",
    "Sidenote: if $\\mathbb{X}$ space is continuous, $x_j$ for both algorithms should be taken from a finite grid $\\mathbb{X}_0$ over $\\mathbb{X}$. For $x$ that does not belong to the grid the value of the function $V$ could be calculated via different approximations: closest neighbor-based, linear, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY\n",
    "\"\"\"\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, env_name, is_slippery=False):\n",
    "        self.env = gym.make(env_name, is_slippery=is_slippery)\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "        self.gamma = 0.95\n",
    "        self.nu = 0.0005\n",
    "        \n",
    "    def return_rewards(self):\n",
    "        return 1.0 in self.rewards.values()\n",
    "\n",
    "    def return_state_values(self):\n",
    "        return tuple(self.values.items())\n",
    "\n",
    "    def _model_transits_rewards(self, num_steps):\n",
    "        \"\"\"\n",
    "\n",
    "        Description: step through the environment to model rewards and transits for all states. Also called \"filling a buffer\".\n",
    "\n",
    "        Args:\n",
    "            * num_steps - num steps to take through env. Should be sufficient to stochastically achieve goal of 1.\n",
    "        \"\"\"\n",
    "        current_state = self.env.reset()\n",
    "\n",
    "        print(\"Modeling rewards and transition probabilities ...\")\n",
    "    \n",
    "        for i in tqdm(range(num_steps)):\n",
    "            # sample random action\n",
    "            action = self.env.action_space.sample()\n",
    "\n",
    "            # take step\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            \n",
    "            # assign rewards for new state\n",
    "            self.rewards[(current_state, action, new_state)] = reward\n",
    "\n",
    "            # log transit from state to new state\n",
    "            self.transits[(current_state, action)][new_state] += 1\n",
    "            \n",
    "            if is_done:\n",
    "                current_state = self.env.reset()\n",
    "            else:\n",
    "                current_state = new_state\n",
    "\n",
    "    def _get_action_value(self, current_state, action):\n",
    "        \"\"\" \n",
    "\n",
    "        Description: Get the value of action\n",
    "\n",
    "        Args:\n",
    "            * State\n",
    "            * Action\n",
    "\n",
    "        Returns:\n",
    "            * Value of current state\n",
    "\n",
    "        \"\"\"\n",
    "        next_state_counts = self.transits[(current_state, action)]\n",
    "        total_transits = sum(next_state_counts.values())\n",
    "        action_value = 0.0\n",
    "        \n",
    "        for next_state, n_transits in next_state_counts.items():\n",
    "            reward = self.rewards[(current_state, action, next_state)]\n",
    "            transit_prob = (n_transits / total_transits)\n",
    "            action_value += transit_prob * (reward + self.gamma * self.values[next_state])\n",
    "        \n",
    "        return action_value\n",
    "\n",
    "    def _get_best_action(self, state):\n",
    "        \"\"\" \n",
    "\n",
    "        Description: get best action for current state\n",
    "\n",
    "        Args:\n",
    "            * state\n",
    "\n",
    "        Returns:\n",
    "            * best action for current state\n",
    "\n",
    "        \"\"\"\n",
    "        action_values = {}\n",
    "\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self._get_action_value(state, action)\n",
    "            action_values[action] = action_value\n",
    "\n",
    "        best_action_value = max(action_values.values())\n",
    "        best_action = max(action_values, key=action_values.get)\n",
    "        \n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Section 4: Problems</h2>\n",
    "\n",
    "### <font color=\"blue\">Problem 1.1 - Value Iteration</font>\n",
    "\n",
    "Implement Value Iteration with the FrozenLake environment.\n",
    "\n",
    "Guidance/hints:\n",
    "* Read Chapter 4 from the class text\n",
    "* Make sure you understand the value iteration algorithm\n",
    "* Test taking steps through the FrozenLake environment by making your own script and executing it in new code cells\n",
    "* Explore the MDP class above\n",
    "* To complete the task, you'll need to make sure that your algorithm is properly calculating the state values. Call the `return_state_values` method to check the value of states. The value of the final state (aka the goal state) is always 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ADD YOUR CODE BETWEEN THE COMMENTS BELOW\n",
    "\"\"\"\n",
    "class ValueIteration(MDP):\n",
    "    def __init__(self, env_name, is_slippery=False):\n",
    "        super().__init__(env_name, is_slippery=is_slippery)\n",
    "        \n",
    "    def _value_iteration(self):\n",
    "        \"\"\" \n",
    "\n",
    "        Description: Perform Value Iteration\n",
    "\n",
    "        \"\"\"\n",
    "        #print(\"Performing value iteration ...\")\n",
    "\n",
    "        delta = 0\n",
    "        iter_count = 1\n",
    "\n",
    "        while True:\n",
    "            ### YOUR SOLUTION BELOW\n",
    "            \n",
    "            ### YOUR SOLUTION ABOVE\n",
    "            \n",
    "            iter_count += 1\n",
    "\n",
    "            if delta < self.nu:\n",
    "                break                   \n",
    "                \n",
    "    def _run_episode(self, render=True):\n",
    "        \"\"\"\n",
    "\n",
    "        Description: perform an episode on the environment\n",
    "\n",
    "        Args\n",
    "            * Render - render env to screen?\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        clear_output()\n",
    "        episode_reward = 0.0\n",
    "        current_state = self.env.reset()\n",
    "\n",
    "        if render:\n",
    "            self.env.render()\n",
    "        \n",
    "        while True:\n",
    "            action = self._get_best_action(current_state)\n",
    "            new_state, step_reward, is_done, _ = self.env.step(action)\n",
    "            \n",
    "            self.rewards[(current_state, action, new_state)] = step_reward\n",
    "            self.transits[(current_state, action)][new_state] += 1\n",
    "            \n",
    "            episode_reward += step_reward\n",
    "            \n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            if is_done:\n",
    "                self.env.reset()\n",
    "                break\n",
    "            \n",
    "            current_state = new_state\n",
    "\n",
    "        print(f\"...Episode completed.\")\n",
    "\n",
    "        return episode_reward\n",
    "        \n",
    "    def run_simulation(self, num_steps = 1000, render=True):\n",
    "        \"\"\" Run training simulation \"\"\"\n",
    "        try:\n",
    "            ### YOUR SOLUTION BELOW\n",
    "            \n",
    "            ### YOUR SOLUTION ABOVE\n",
    "            \n",
    "            episode_reward = self._run_episode(render=render)\n",
    "\n",
    "            if episode_reward > 0.85:\n",
    "                print(f\"Environment solved.\")\n",
    "            else:\n",
    "                clear_output()\n",
    "                print(f\"Failed to solve environment.\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"...Cancelling...\")\n",
    "            \n",
    "        except NotImplementedError:\n",
    "            print(f\"Your solution is incomplete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run simulation\n",
    "\n",
    "Execute the cell below. If your code is correct, you will see the environment render the agent taking steps. The final cell will be 'G'. \n",
    "\n",
    "**Note**: Running the simulation below with a large enough `num_steps` hyperparameter is vital to the value iteration algorithm working successfully. This is because the modeling process, which is stochastic (via `_model_transits_rewards`), requires sufficient iterations to reach the goal-state and acquire the reward in the environment, as well as to make enough transits to all possible states to make value iteration numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "agent1 = ValueIteration(\"FrozenLake-v0\")\n",
    "agent1.run_simulation(num_steps = 3000)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Duration of execution: {end_time-start_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#agent1.return_state_values()\n",
    "#agent1.env.observation_space.n\n",
    "agent1.values[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State values?\n",
    "\n",
    "In a 4x4 grid of FrozenLake, there are 16 states. Now let's look at their values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_values = agent1.return_state_values()\n",
    "state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = agent1.return_rewards()\n",
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 1.1. Make sure you defined the necessary variable above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "from grading_utilities import AnswerTracker\n",
    "asgn1_answers = AnswerTracker()\n",
    "rewards_values = agent1.return_rewards()\n",
    "asgn1_answers.record('problem_1-1', {'state_values': state_values, 'rewards': rewards_values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Problem 1.2 - Policy Iteration</font>\n",
    "\n",
    "Implement Policy Iteration on the FrozenLake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration(MDP):\n",
    "    def __init__(self, env_name, is_slippery=False):\n",
    "        super().__init__(env_name, is_slippery=is_slippery)\n",
    "        self.policy = collections.defaultdict(int)\n",
    "        \n",
    "    def return_policy(self):\n",
    "        return tuple(self.policy.items())\n",
    "        \n",
    "    def _policy_iteration(self):\n",
    "        \"\"\" \n",
    "        \n",
    "        Description: Perform policy iteration. Consists of 2 parts: policy evaluation and policy improvement. See Sutton & Barto, RL: An Introduction, page 80.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #-------------------#\n",
    "        # policy evaluation #\n",
    "        #-------------------#\n",
    "\n",
    "        delta = 0\n",
    "        iter_count = 1\n",
    "\n",
    "        while True:\n",
    "            ### YOUR SOLUTION BELOW\n",
    "            \n",
    "            ### YOUR SOLUTION ABOVE\n",
    "            \n",
    "            iter_count += 1\n",
    "\n",
    "            if delta < self.nu:\n",
    "                break\n",
    "\n",
    "        #--------------------#\n",
    "        # policy improvement #\n",
    "        #--------------------#\n",
    "\n",
    "        policy_stable = np.zeros((self.env.observation_space.n), dtype=bool)\n",
    "\n",
    "        ### YOUR SOLUTION BELOW\n",
    "        \n",
    "        ### YOUR SOLUTION ABOVE\n",
    "\n",
    "        return policy_stable\n",
    "    \n",
    "    def _run_episode(self, render=True):\n",
    "        \"\"\"\n",
    "\n",
    "        Description: runs an episode on the environment after policy iteration.\n",
    "\n",
    "        Args:\n",
    "            * Render - render env to screen?\n",
    "\n",
    "        \"\"\"\n",
    "        clear_output()\n",
    "        episode_reward = 0.0\n",
    "        current_state = self.env.reset()\n",
    "\n",
    "        if render:\n",
    "            self.env.render()\n",
    "        \n",
    "        while True:\n",
    "            action = self.policy[current_state]\n",
    "            new_state, step_reward, is_done, _ = self.env.step(action)\n",
    "            \n",
    "            self.rewards[(current_state, action, new_state)] = step_reward\n",
    "            self.transits[(current_state, action)][new_state] += 1\n",
    "            \n",
    "            episode_reward += step_reward\n",
    "            \n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            if is_done:\n",
    "                self.env.reset()\n",
    "                break\n",
    "            \n",
    "            current_state = new_state\n",
    "            \n",
    "        print(f\"...Episode completed.\")\n",
    "\n",
    "        return episode_reward\n",
    "        \n",
    "    def run_simulation(self, num_steps = 2000, render=True):\n",
    "        \"\"\" Run training simulation \"\"\"\n",
    "        try:\n",
    "            self._model_transits_rewards(num_steps)\n",
    "\n",
    "            while True:\n",
    "                policy_stable = self._policy_iteration()\n",
    "\n",
    "                if policy_stable.all() == True:\n",
    "                    break\n",
    "\n",
    "            episode_reward = self._run_episode(render=render)\n",
    "            \n",
    "            if episode_reward > 0.85:\n",
    "                print(f\"Environment solved.\")\n",
    "            else:\n",
    "                clear_output()\n",
    "                print(f\"Failed to solve environment.\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"...Cancelling...\")\n",
    "            \n",
    "        except NotImplementedError:\n",
    "            print(f\"Your solution is incomplete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run simulation\n",
    "\n",
    "As discussed above, beware of the num_steps hyperparam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "agent2 = PolicyIteration(\"FrozenLake-v0\")\n",
    "agent2.run_simulation(num_steps = 3000)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Duration of execution: {end_time-start_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 1.2. Make sure you defined the necessary variable above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_values = agent2.return_state_values()\n",
    "policy_values = agent2.return_policy()\n",
    "\n",
    "### GRADING DO NOT MODIFY\n",
    "asgn1_answers.record('problem_1-2', {'state_values': state_values, 'policy_values': policy_values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Problem 1.3 - FrozenLake8x8</font>\n",
    "\n",
    "Let's try the 8x8 version of FrozenLake and compare which algorithm is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# value iteration\n",
    "agent3 = ValueIteration(\"FrozenLake8x8-v0\")\n",
    "agent3.run_simulation(render = False, num_steps = 50000)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"VI: Duration of execution: {end_time-start_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "agent3._value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# policy iteration\n",
    "agent4 = PolicyIteration(\"FrozenLake8x8-v0\")\n",
    "agent4.run_simulation(render = False, num_steps = 50000)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"PI: Duration of execution: {end_time-start_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "agent4._policy_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which algorithm is faster?\n",
    "\n",
    "Which algorithm do you think is faster, on average, policy iteration (PI) or value iteration (VI)? The answer should be clear, if the algorithms are implemented correctly; try to find out, why is it so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Change None to 'PI' or 'VI' below\n",
    "\"\"\"\n",
    "### YOUR ANSWER BELOW\n",
    "problem_13_answer = ''\n",
    "### YOUR ANSWER ABOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 1.3. Make sure you defined the necessary variable above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "asgn1_answers.record('problem_1-3', problem_13_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading: Submit your answers</font>\n",
    "Enter your first and last name in the cell below and then run it to save your answers for this lab to a JSON file. The file is saved to the same directory as this notebook. After the file is created, upload the JSON file to the assignment page on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asgn1_answers.print_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assignment_name = \"asgn_1\"\n",
    "first_name = \"\"\n",
    "last_name = \"\"\n",
    "\n",
    "asgn1_answers.save_to_json(assignment_name, first_name, last_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "\n",
    "Reach out to Ilya Osokin (@elijahmipt) on Telegram.\n",
    "\n",
    "## Sources\n",
    "\n",
    "***\n",
    "\n",
    "<sup>[1]</sup> Ng, A. Stanford University, CS229 Notes: Reinforcement Learning and Control.\n",
    "\n",
    "<sup>[2]</sup> Barnabás Póczos, Carnegie Mellon, Introduction To Machine Learning: Reinforcement Learning (Course).\n",
    "\n",
    "<sup>[3]</sup> **Sutton, R. S., Barto, A. G. (2018 ). Reinforcement Learning: An Introduction. The MIT Press.** \n",
    "\n",
    "<sup>[4]</sup> OpenAI: Spinning Up. Retrieved from https://spinningup.openai.com/en/latest/spinningup/rl_intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
